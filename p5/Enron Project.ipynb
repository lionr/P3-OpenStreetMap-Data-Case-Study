{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Submission Free-Response Questions\n",
    "\n",
    "A critical part of machine learning is making sense of your analysis process and communicating it to others. The questions below will help us understand your decision-making process and allow us to give feedback on your project. Please answer each question; your answers should be about 1-2 paragraphs per question. If you find yourself writing much more than that, take a step back and see if you can simplify your response!\n",
    "\n",
    "When your evaluator looks at your responses, he or she will use a specific list of rubric items to assess your answers. Here is the link to that rubric: [Link] Each question has one or more specific rubric items associated with it, so before you submit an answer, take a look at that part of the rubric. If your response does not meet expectations for all rubric points, you will be asked to revise and resubmit your project. Make sure that your responses are detailed enough that the evaluator will be able to understand the steps you took and your thought processes as you went through the data analysis.\n",
    "\n",
    "Once you’ve submitted your responses, your coach will take a look and may ask a few more focused follow-up questions on one or more of your answers.  \n",
    "\n",
    "We can’t wait to see what you’ve put together for this project!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "1.Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the project is to create a machine learning model that could separate out the POIs. Machine Learning can predict the POI/Non-POI feature after \"learning\" features in the training data.\n",
    "\n",
    "The Enron dataset is comprised of email and financial dataset collected and prepared by the CALO Project. It contains data from about 150 users, mostly senior management of Enron, orgnized into folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Overview\n",
    "- 146 points, each representing a person (2 are not people)\n",
    "- 18 of these pionts are labeled as a POI and 128 as Non-POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 146 entries, ALLEN PHILLIP K to YEAP SOON\n",
      "Data columns (total 21 columns):\n",
      "bonus                        146 non-null object\n",
      "deferral_payments            146 non-null object\n",
      "deferred_income              146 non-null object\n",
      "director_fees                146 non-null object\n",
      "email_address                146 non-null object\n",
      "exercised_stock_options      146 non-null object\n",
      "expenses                     146 non-null object\n",
      "from_messages                146 non-null object\n",
      "from_poi_to_this_person      146 non-null object\n",
      "from_this_person_to_poi      146 non-null object\n",
      "loan_advances                146 non-null object\n",
      "long_term_incentive          146 non-null object\n",
      "other                        146 non-null object\n",
      "poi                          146 non-null object\n",
      "restricted_stock             146 non-null object\n",
      "restricted_stock_deferred    146 non-null object\n",
      "salary                       146 non-null object\n",
      "shared_receipt_with_poi      146 non-null object\n",
      "to_messages                  146 non-null object\n",
      "total_payments               146 non-null object\n",
      "total_stock_value            146 non-null object\n",
      "dtypes: object(21)\n",
      "memory usage: 25.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Basic information of the data\n",
    "data_df = pd.DataFrame(data_dict).T\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>poi</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALLEN PHILLIP K</th>\n",
       "      <td>4175000</td>\n",
       "      <td>2869717</td>\n",
       "      <td>-3081055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1729541</td>\n",
       "      <td>13868</td>\n",
       "      <td>2195</td>\n",
       "      <td>47</td>\n",
       "      <td>65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>304805</td>\n",
       "      <td>152</td>\n",
       "      <td>False</td>\n",
       "      <td>126027</td>\n",
       "      <td>-126027</td>\n",
       "      <td>201955</td>\n",
       "      <td>1407</td>\n",
       "      <td>2902</td>\n",
       "      <td>4484442</td>\n",
       "      <td>1729541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BADUM JAMES P</th>\n",
       "      <td>NaN</td>\n",
       "      <td>178980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257817</td>\n",
       "      <td>3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182466</td>\n",
       "      <td>257817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BANNANTINE JAMES M</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4046157</td>\n",
       "      <td>56301</td>\n",
       "      <td>29</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864523</td>\n",
       "      <td>False</td>\n",
       "      <td>1757552</td>\n",
       "      <td>-560222</td>\n",
       "      <td>477</td>\n",
       "      <td>465</td>\n",
       "      <td>566</td>\n",
       "      <td>916197</td>\n",
       "      <td>5243487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAXTER JOHN C</th>\n",
       "      <td>1200000</td>\n",
       "      <td>1295738</td>\n",
       "      <td>-1386055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6680544</td>\n",
       "      <td>11200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1586055</td>\n",
       "      <td>2660303</td>\n",
       "      <td>False</td>\n",
       "      <td>3942714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>267102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5634343</td>\n",
       "      <td>10623258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAY FRANKLIN R</th>\n",
       "      <td>400000</td>\n",
       "      <td>260455</td>\n",
       "      <td>-201641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>False</td>\n",
       "      <td>145796</td>\n",
       "      <td>-82782</td>\n",
       "      <td>239671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827696</td>\n",
       "      <td>63014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      bonus deferral_payments deferred_income director_fees  \\\n",
       "ALLEN PHILLIP K     4175000           2869717        -3081055           NaN   \n",
       "BADUM JAMES P           NaN            178980             NaN           NaN   \n",
       "BANNANTINE JAMES M      NaN               NaN           -5104           NaN   \n",
       "BAXTER JOHN C       1200000           1295738        -1386055           NaN   \n",
       "BAY FRANKLIN R       400000            260455         -201641           NaN   \n",
       "\n",
       "                   exercised_stock_options expenses from_messages  \\\n",
       "ALLEN PHILLIP K                    1729541    13868          2195   \n",
       "BADUM JAMES P                       257817     3486           NaN   \n",
       "BANNANTINE JAMES M                 4046157    56301            29   \n",
       "BAXTER JOHN C                      6680544    11200           NaN   \n",
       "BAY FRANKLIN R                         NaN   129142           NaN   \n",
       "\n",
       "                   from_poi_to_this_person from_this_person_to_poi  \\\n",
       "ALLEN PHILLIP K                         47                      65   \n",
       "BADUM JAMES P                          NaN                     NaN   \n",
       "BANNANTINE JAMES M                      39                       0   \n",
       "BAXTER JOHN C                          NaN                     NaN   \n",
       "BAY FRANKLIN R                         NaN                     NaN   \n",
       "\n",
       "                   loan_advances long_term_incentive    other    poi  \\\n",
       "ALLEN PHILLIP K              NaN              304805      152  False   \n",
       "BADUM JAMES P                NaN                 NaN      NaN  False   \n",
       "BANNANTINE JAMES M           NaN                 NaN   864523  False   \n",
       "BAXTER JOHN C                NaN             1586055  2660303  False   \n",
       "BAY FRANKLIN R               NaN                 NaN       69  False   \n",
       "\n",
       "                   restricted_stock restricted_stock_deferred  salary  \\\n",
       "ALLEN PHILLIP K              126027                   -126027  201955   \n",
       "BADUM JAMES P                   NaN                       NaN     NaN   \n",
       "BANNANTINE JAMES M          1757552                   -560222     477   \n",
       "BAXTER JOHN C               3942714                       NaN  267102   \n",
       "BAY FRANKLIN R               145796                    -82782  239671   \n",
       "\n",
       "                   shared_receipt_with_poi to_messages total_payments  \\\n",
       "ALLEN PHILLIP K                       1407        2902        4484442   \n",
       "BADUM JAMES P                          NaN         NaN         182466   \n",
       "BANNANTINE JAMES M                     465         566         916197   \n",
       "BAXTER JOHN C                          NaN         NaN        5634343   \n",
       "BAY FRANKLIN R                         NaN         NaN         827696   \n",
       "\n",
       "                   total_stock_value  \n",
       "ALLEN PHILLIP K              1729541  \n",
       "BADUM JAMES P                 257817  \n",
       "BANNANTINE JAMES M           5243487  \n",
       "BAXTER JOHN C               10623258  \n",
       "BAY FRANKLIN R                 63014  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.drop('email_address', axis=1, inplace=True)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Scattering the dataset based on the salary and bonus feature of the points, I find two outliers: TOTAL and THE TRAVEL AGENCY IN THE PARK, which apparently are not people.\n",
    "\n",
    "Also, I find there's a man called LOCKHART EUGENE E and all the values are NaN, so that's a totally invalid point and I need to remove it from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'director_fees': 'NaN',\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 'NaN',\n",
       " 'poi': False,\n",
       " 'restricted_stock': 'NaN',\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'salary': 'NaN',\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 'NaN',\n",
       " 'total_stock_value': 'NaN'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pop out two outliers and leaving 144 useful datasets\n",
    "data_dict.pop('TOTAL')\n",
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK')\n",
    "data_dict.pop('LOCKHART EUGENE E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "2.What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Features Used\n",
    "\n",
    "The algorithm I finally used was DecisionTreeClassifier and I used SelectKBest to help me select the following nine features in my POI identifier, ordered by importances descending (the first decimal number of each row):\n",
    "\n",
    "- feature no. 1 shared_receipt_with_poi 0.403561962807 15.7854252775\n",
    "- feature no. 2 from_poi_to_this_person_ratio 0.147344335528 2.48838097173\n",
    "- feature no. 3 salary 0.145954110897 17.7678544529\n",
    "- feature no. 4 bonus 0.0950254426577 34.2129648303\n",
    "- feature no. 5 from_this_person_to_poi_ratio 0.0942268021027 0.215888289188\n",
    "- feature no. 6 bonus_salary_ratio 0.0780089283869 22.1067164085\n",
    "- feature no. 7 exercised_stock_options 0.0358784176204 16.9328653375\n",
    "- feature no. 8 total_stock_value 0.0 16.8651432616\n",
    "\n",
    "shared_receipt_with_poi, from_poi_to_this_person_ratio, salary are the most important features.\n",
    "### Selection Process\n",
    "I used SelectKBest in a pipeline with grid search to find the K best features. SelectKBest removes all but the k hightest scoring features. The number, k, was chosen by an exhaustive grid search decided by the \"f1\" scoring estimator. \n",
    "\n",
    "### Features Scaling\n",
    "I choose two classifier: SVM and DecisionTreeClassifier\n",
    "\n",
    "According to [A Practical Guide to Support Vector Classification ](http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf), scaling before applying SVM is very important. The main advantage of scaling is to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges. Another advantage is to avoid numerical difficulties during the calculation. Because kernel values usually depend on the inner products of feature vectors, e.g. the linear kernel and the polynomial ker- nel, large attribute values might cause numerical problems.\n",
    "\n",
    "However, Scaling isn't required for tree-based algorithms because the splitting of the data is based on a threshold value. \n",
    "### Features Engineered\n",
    "I add threee features to the dataset:\n",
    "- bonus_salary_ratio\n",
    "- from_poi_to_this_person_ratio\n",
    "- from_this_person_to_poi_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "3.What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately I used DecisionTreeClassifier. I also tried SVM.\n",
    "- The decision tree classifier had a best performace, with a precision of 0.38050 and a recall of 0.38493, both above the 0.3 threshold.\n",
    "- The SVM had a precision of 0.59259 but a recall of 0.05600.\n",
    "\n",
    "The SVM classifier is not a good fit for the extremely unbalanced classes in the Enron dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questionn 4\n",
    "4.What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tuning the parameters of an algorithm means adjusting the parameters in a certain way to achieve the optimal performance. If I don't tune the parameters, definitely I will end up using the defaults, which will not result in an optimal performance. In other words, the data can't be learned well and the performance may suffer.\n",
    "\n",
    "I simply use GridSearchCV to help me to find the best parameters combination. The algorithm performance can be measured in many ways such as accuracy, precision and recall etc. \n",
    "\n",
    "For the chosen DecisionTreeClassifier, I have tried many combinations of parameters. It can be seen as below:\n",
    "- dtc__criterion = ['gini', 'entropy'],\n",
    "- dtc__min_samples_split = [2, 4, 6, 8, 10, 20],\n",
    "- dtc__max_depth = [None, 5, 10, 15, 20],\n",
    "- dtc__max_features = [None, 'sqrt', 'log2', 'auto'],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "5.What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived. The main purpose of using the testing data set is to test the generalization ability of a trained model.\n",
    "\n",
    "A classic mistake is to test your algorithm on the same data you trained on. In this situation your accuracy is 100%. So we have to separate our data into two parts--the training set and the testing set.\n",
    "\n",
    "I also tried to use StratifiedShuffleSplit as an alternative to gauge the algorithm's performance. It can create multiple datasets to help to promote the accuracy (the enron dataset itself is so small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "6.Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Two important evaluation metrics are precision and recall. The average performance for them are 0.38946 and 0.38050, respectively. \n",
    "\n",
    "Details below:\n",
    "- Accuracy: 0.83787      \n",
    "- Precision: 0.38946      \n",
    "- Recall: 0.38050 \n",
    "- F1: 0.38493     \n",
    "- F2: 0.38226\n",
    "\n",
    "To understand it in a simpler way:\n",
    "- Precisions refer to the ratio of correct positive predicitons made out of all positive predicitons we made. Let's see this algorithm's performance to clearly understand it: we made 1954 postive predicitons, but only 761 are correct, so precision = 761/1954 = 0.38946\n",
    "- Recall refers to the ratio of correct positive predictions made out of all actual postive points. In reality, the number of the total positive points are 2000, in which 761 we predicted correctly, so recall is 761/2000 = 0.38050\n",
    "\n",
    "The total predictions are 15000 points, in which 761 are true positives, 1193 are false positives, 1239 are false negatives and 11807 are true negatives.\n",
    "\n",
    "It's easy to see there'e a tradeoff between precision and recall, that's why we use \"f1\" as the parameter of scoring when applying gridSeachCV.\n",
    "\n",
    "F1 is the harmonic mean of precision and recall:\n",
    "- f1 = 2 * true_positives/(2*true_positives + false_positives+false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cs.cmu.edu/~./enron/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://link.springer.com/referenceworkentry/10.1007%2F978-1-4419-9863-7_233"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/F1_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
